import torch
import clip
import numpy as np
import json
import os
from PIL import Image
from tqdm import tqdm
import pickle

class CLIPFeatureExtractor:
    """
    TrÃ­ch xuáº¥t feature vectors tá»« áº£nh sá»­ dá»¥ng CLIP model
    """
    def __init__(self, model_name="ViT-B/32", device=None):
        """
        Args:
            model_name: TÃªn mÃ´ hÃ¬nh CLIP ("RN50", "ViT-B/32", "ViT-B/16", "ViT-L/14")
            device: Device Ä‘á»ƒ cháº¡y model (cuda/cpu)
        """
        self.device = device or ("cuda" if torch.cuda.is_available() else "cpu")
        print(f"ğŸ”§ Äang táº£i mÃ´ hÃ¬nh CLIP {model_name} trÃªn {self.device}...")
        
        self.model, self.preprocess = clip.load(model_name, device=self.device)
        self.model.eval()
        
        print(f"âœ… ÄÃ£ táº£i mÃ´ hÃ¬nh CLIP thÃ nh cÃ´ng!")
        print(f"   - Model: {model_name}")
        print(f"   - Device: {self.device}")
        print(f"   - Feature dimension: {self.model.visual.output_dim}")
    
    def extract_features(self, image_path):
        """
        TrÃ­ch xuáº¥t feature vector tá»« má»™t áº£nh
        
        Args:
            image_path: ÄÆ°á»ng dáº«n Ä‘áº¿n áº£nh
            
        Returns:
            Feature vector (numpy array)
        """
        try:
            # Load vÃ  preprocess áº£nh
            image = Image.open(image_path).convert('RGB')
            image_input = self.preprocess(image).unsqueeze(0).to(self.device)
            
            # TrÃ­ch xuáº¥t features
            with torch.no_grad():
                features = self.model.encode_image(image_input)
                # Normalize features (quan trá»ng cho similarity search)
                features = features / features.norm(dim=-1, keepdim=True)
            
            return features.cpu().numpy().flatten()
        
        except Exception as e:
            print(f"âŒ Lá»—i khi xá»­ lÃ½ áº£nh {image_path}: {e}")
            return None
    
    def extract_batch_features(self, image_paths, batch_size=32):
        """
        TrÃ­ch xuáº¥t features cho nhiá»u áº£nh (batch processing)
        
        Args:
            image_paths: List Ä‘Æ°á»ng dáº«n áº£nh
            batch_size: Sá»‘ áº£nh xá»­ lÃ½ cÃ¹ng lÃºc
            
        Returns:
            List of feature vectors
        """
        all_features = []
        
        for i in tqdm(range(0, len(image_paths), batch_size), desc="Extracting features"):
            batch_paths = image_paths[i:i+batch_size]
            batch_images = []
            valid_indices = []
            
            # Load vÃ  preprocess batch
            for idx, path in enumerate(batch_paths):
                try:
                    image = Image.open(path).convert('RGB')
                    image_input = self.preprocess(image)
                    batch_images.append(image_input)
                    valid_indices.append(idx)
                except Exception as e:
                    print(f"âš ï¸ Bá» qua áº£nh {path}: {e}")
                    all_features.append(None)
            
            if len(batch_images) == 0:
                continue
            
            # Stack thÃ nh batch tensor
            batch_tensor = torch.stack(batch_images).to(self.device)
            
            # Extract features
            with torch.no_grad():
                features = self.model.encode_image(batch_tensor)
                features = features / features.norm(dim=-1, keepdim=True)
                features_np = features.cpu().numpy()
            
            # ThÃªm vÃ o káº¿t quáº£
            feature_idx = 0
            for idx in range(len(batch_paths)):
                if idx in valid_indices:
                    all_features.append(features_np[feature_idx])
                    feature_idx += 1
        
        return all_features

def process_cifar100_features(metadata_file='product_metadata.json', 
                               output_dir='./features',
                               model_name="ViT-B/32",
                               batch_size=32):
    """
    Xá»­ lÃ½ toÃ n bá»™ CIFAR-100 vÃ  trÃ­ch xuáº¥t features
    
    Args:
        metadata_file: File JSON chá»©a metadata sáº£n pháº©m
        output_dir: ThÆ° má»¥c lÆ°u features
        model_name: TÃªn mÃ´ hÃ¬nh CLIP
        batch_size: Batch size cho feature extraction
    """
    print("=" * 70)
    print("ğŸš€ Báº®T Äáº¦U TRÃCH XUáº¤T FEATURES Tá»ª CIFAR-100")
    print("=" * 70)
    
    # 1. Load metadata
    print(f"\nğŸ“‚ Äang Ä‘á»c metadata tá»« {metadata_file}...")
    with open(metadata_file, 'r', encoding='utf-8') as f:
        products = json.load(f)
    print(f"   âœ… ÄÃ£ load {len(products)} sáº£n pháº©m")
    
    # 2. Khá»Ÿi táº¡o CLIP model
    extractor = CLIPFeatureExtractor(model_name=model_name)
    feature_dim = extractor.model.visual.output_dim
    
    # 3. Chuáº©n bá»‹ dá»¯ liá»‡u
    image_paths = [p['image_path'] for p in products]
    product_ids = [p['product_id'] for p in products]
    
    # 4. TrÃ­ch xuáº¥t features
    print(f"\nğŸ” Äang trÃ­ch xuáº¥t features cho {len(image_paths)} áº£nh...")
    print(f"   - Batch size: {batch_size}")
    print(f"   - Feature dimension: {feature_dim}")
    
    features = extractor.extract_batch_features(image_paths, batch_size=batch_size)
    
    # 5. Xá»­ lÃ½ káº¿t quáº£
    valid_features = []
    valid_product_ids = []
    valid_indices = []
    
    for idx, (feat, prod_id) in enumerate(zip(features, product_ids)):
        if feat is not None:
            valid_features.append(feat)
            valid_product_ids.append(prod_id)
            valid_indices.append(idx)
    
    features_array = np.array(valid_features, dtype=np.float32)
    
    print(f"\nâœ… HoÃ n thÃ nh trÃ­ch xuáº¥t features!")
    print(f"   - Sá»‘ features thÃ nh cÃ´ng: {len(valid_features)}/{len(products)}")
    print(f"   - Shape: {features_array.shape}")
    print(f"   - Memory size: {features_array.nbytes / 1024 / 1024:.2f} MB")
    
    # 6. LÆ°u features
    os.makedirs(output_dir, exist_ok=True)
    
    # LÆ°u features array
    features_file = os.path.join(output_dir, 'features.npy')
    np.save(features_file, features_array)
    print(f"\nğŸ’¾ ÄÃ£ lÆ°u features vÃ o: {features_file}")
    
    # LÆ°u product IDs mapping
    ids_file = os.path.join(output_dir, 'product_ids.pkl')
    with open(ids_file, 'wb') as f:
        pickle.dump(valid_product_ids, f)
    print(f"ğŸ’¾ ÄÃ£ lÆ°u product IDs vÃ o: {ids_file}")
    
    # LÆ°u metadata cho features
    metadata = {
        'model_name': model_name,
        'feature_dim': feature_dim,
        'total_products': len(products),
        'valid_features': len(valid_features),
        'feature_shape': features_array.shape,
        'product_ids': valid_product_ids
    }
    
    metadata_file_out = os.path.join(output_dir, 'features_metadata.json')
    with open(metadata_file_out, 'w', encoding='utf-8') as f:
        json.dump(metadata, f, ensure_ascii=False, indent=2)
    print(f"ğŸ’¾ ÄÃ£ lÆ°u metadata vÃ o: {metadata_file_out}")
    
    # 7. Táº¡o index mapping (product_id -> feature_index)
    id_to_index = {prod_id: idx for idx, prod_id in enumerate(valid_product_ids)}
    index_file = os.path.join(output_dir, 'id_to_index.pkl')
    with open(index_file, 'wb') as f:
        pickle.dump(id_to_index, f)
    print(f"ğŸ’¾ ÄÃ£ lÆ°u index mapping vÃ o: {index_file}")
    
    # 8. Thá»‘ng kÃª
    print("\n" + "=" * 70)
    print("ğŸ“Š THá»NG KÃŠ")
    print("=" * 70)
    print(f"Model: {model_name}")
    print(f"Feature dimension: {feature_dim}")
    print(f"Total products: {len(products)}")
    print(f"Successfully extracted: {len(valid_features)}")
    print(f"Failed: {len(products) - len(valid_features)}")
    print(f"Feature matrix shape: {features_array.shape}")
    print(f"Memory usage: {features_array.nbytes / 1024 / 1024:.2f} MB")
    
    # PhÃ¢n tÃ­ch theo category
    category_stats = {}
    for idx in valid_indices:
        cat = products[idx]['category']
        category_stats[cat] = category_stats.get(cat, 0) + 1
    
    print(f"\nğŸ“ˆ Thá»‘ng kÃª theo category (top 10):")
    sorted_cats = sorted(category_stats.items(), key=lambda x: x[1], reverse=True)[:10]
    for cat, count in sorted_cats:
        print(f"   - {cat}: {count} products")
    
    print("\n" + "=" * 70)
    print("âœ¨ HOÃ€N THÃ€NH!")
    print("=" * 70)
    
    return features_array, valid_product_ids, id_to_index

def test_feature_similarity(features_file='./features/features.npy',
                           ids_file='./features/product_ids.pkl',
                           metadata_file='product_metadata.json',
                           test_product_id='train_00000'):
    """
    Test tÃ­nh nÄƒng tÃ¬m kiáº¿m sáº£n pháº©m tÆ°Æ¡ng tá»±
    
    Args:
        features_file: File chá»©a feature vectors
        ids_file: File chá»©a product IDs
        metadata_file: File metadata sáº£n pháº©m
        test_product_id: ID sáº£n pháº©m Ä‘á»ƒ test
    """
    print("\n" + "=" * 70)
    print("ğŸ§ª TEST TÃŒM KIáº¾M Sáº¢N PHáº¨M TÆ¯Æ NG Tá»°")
    print("=" * 70)
    
    # Load data
    print("\nğŸ“‚ Äang load dá»¯ liá»‡u...")
    features = np.load(features_file)
    with open(ids_file, 'rb') as f:
        product_ids = pickle.load(f)
    with open(metadata_file, 'r', encoding='utf-8') as f:
        products = json.load(f)
    
    # Táº¡o mapping
    id_to_product = {p['product_id']: p for p in products}
    id_to_index = {prod_id: idx for idx, prod_id in enumerate(product_ids)}
    
    # Láº¥y feature cá»§a sáº£n pháº©m test
    if test_product_id not in id_to_index:
        print(f"âŒ KhÃ´ng tÃ¬m tháº¥y product_id: {test_product_id}")
        return
    
    test_idx = id_to_index[test_product_id]
    test_feature = features[test_idx]
    test_product = id_to_product[test_product_id]
    
    print(f"\nğŸ” Sáº£n pháº©m test:")
    print(f"   - ID: {test_product['product_id']}")
    print(f"   - Name: {test_product['product_name']}")
    print(f"   - Category: {test_product['category']}")
    print(f"   - Price: {test_product['price']:,} VNÄ")
    
    # TÃ­nh similarity vá»›i táº¥t cáº£ sáº£n pháº©m khÃ¡c
    print(f"\nğŸ” TÃ­nh toÃ¡n similarity vá»›i {len(features)} sáº£n pháº©m...")
    similarities = np.dot(features, test_feature)
    
    # Láº¥y top 10 sáº£n pháº©m tÆ°Æ¡ng tá»± (bá» qua chÃ­nh nÃ³)
    top_k = 11  # Láº¥y 11 Ä‘á»ƒ bá» chÃ­nh nÃ³
    top_indices = np.argsort(similarities)[::-1][:top_k]
    
    print(f"\nğŸ† TOP 10 Sáº¢N PHáº¨M TÆ¯Æ NG Tá»°:")
    print("-" * 70)
    
    for rank, idx in enumerate(top_indices, 1):
        if idx == test_idx:
            continue  # Skip chÃ­nh nÃ³
        
        similar_id = product_ids[idx]
        similar_product = id_to_product[similar_id]
        similarity_score = similarities[idx]
        
        print(f"\n{rank}. {similar_product['product_name']}")
        print(f"   - Category: {similar_product['category']}")
        print(f"   - Price: {similar_product['price']:,} VNÄ")
        print(f"   - Similarity: {similarity_score:.4f}")
        print(f"   - Same category: {'âœ…' if similar_product['category'] == test_product['category'] else 'âŒ'}")

def main():
    """
    HÃ m main Ä‘á»ƒ cháº¡y toÃ n bá»™ pipeline
    """
    # Cáº¥u hÃ¬nh
    CONFIG = {
        'metadata_file': 'product_metadata.json',
        'output_dir': './features',
        'model_name': 'ViT-B/32',  # CÃ³ thá»ƒ thay Ä‘á»•i: "RN50", "ViT-B/16", "ViT-L/14"
        'batch_size': 64,  # TÃ¹y theo GPU memory
    }
    
    # 1. TrÃ­ch xuáº¥t features
    features, product_ids, id_to_index = process_cifar100_features(**CONFIG)
    
    # 2. Test similarity search
    print("\n")
    test_feature_similarity(
        features_file=os.path.join(CONFIG['output_dir'], 'features.npy'),
        ids_file=os.path.join(CONFIG['output_dir'], 'product_ids.pkl'),
        metadata_file=CONFIG['metadata_file'],
        test_product_id=product_ids[0]  # Test vá»›i sáº£n pháº©m Ä‘áº§u tiÃªn
    )

if __name__ == "__main__":
    main()